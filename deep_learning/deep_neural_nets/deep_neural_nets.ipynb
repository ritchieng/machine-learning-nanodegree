{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Model Complexity**\n",
    "- If we have N inputs and K outputs, we would have:\n",
    "    - (N+1)K parameters\n",
    "    - ![](dnn.png)\n",
    "- Limitation\n",
    "    - $y = x_1 + x_2$ can be represented well\n",
    "    - $y = x_1 * x_2$ cannot be represented well\n",
    "- Benefits\n",
    "    - Derivatives are constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rectified Linear Units (ReLUs)**\n",
    "- This is a non-linear function.\n",
    "- ![](dnn2.png)\n",
    "    - Derivatives are nicely represented too.\n",
    "        - ![](dnn3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network of ReLUs: Neural Network**\n",
    "- We can do a logistic classifier and insert a ReLU to make a non-linear model.\n",
    "    - ![](dnn4.png)\n",
    "    - H: number of RELU units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Layer Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dnn5.png)\n",
    "1. The first layer effectively consists of the set of weights and biases applied to X and passed through ReLUs. The output of this layer is fed to the next one, but is not observable outside the network, hence it is known as a hidden layer.\n",
    "2. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities.\n",
    "    - A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking Simple Operations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dnn7.png)\n",
    "- We can compute derivative of function by taking product of derivatives of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "![](dnn8.png)\n",
    "- Forward-propagation\n",
    "    - You will have data X flowing through your NN to produce Y.\n",
    "- Back-propagation\n",
    "    - Your labelled data Y flows backward to calculate \"errors\" of our calculations.\n",
    "    - You will be calculating the gradients (\"errors\"), multiply it by a learning rate, and use it to update our weights.\n",
    "    - We will be doing this many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Deeper**\n",
    "- It is better go deeper than increasing the size of the hidden layers (by adding more nodes) \n",
    "    - It gets hard to train.\n",
    "- We should go deeper by adding more hidden layers.\n",
    "    - You would reap parameter efficiencies.\n",
    "    - However you need large datasets.\n",
    "    - Also, deep models can capture certain structures well such as the following.\n",
    "        - ![](dnn9.png)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization**\n",
    "- We normally train networks that are bigger than our data. \n",
    "    - Then we try to prevent overfitting with 2 methods.\n",
    "        - Early termination\n",
    "            - ![](dnn10.png)\n",
    "        - Regularization\n",
    "            - Applying artificial constraints.\n",
    "            - Implicitly reduce number of free parameters while enabling us to optimize. \n",
    "            - L2 Regularization\n",
    "                - We add another term to the loss that penalizes large weights. \n",
    "                - This is simple because we just add to our loss.\n",
    "                - ![](dnn11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2 Regularization's Derivative**\n",
    "- The norm of w is the sum of squares of the elements in the vector.\n",
    "    - The equation:\n",
    "        - ![](dnn12.png)\n",
    "    - The derivative:\n",
    "        - $ (\\frac {1}{2} w^2)' = w $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2 Regularizatin: Dropout**\n",
    "- Your input goes through an activation function.\n",
    "    - During the activation function, we randomly take half of the data and set to 0. \n",
    "    - We do this multiple times.\n",
    "        - ![](dnn13.png)\n",
    "- We are forced to learn redundant information.\n",
    "    - It's like a game of whack-a-mole.\n",
    "    - There's always one or more that represents the same thing.\n",
    "        - ![](dnn14.png)\n",
    "- Benefits\n",
    "    - It prevents overfitting.\n",
    "    - It makes network act like it's taking a consensus of an ensemble of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout during Evaluation**\n",
    "- We would take the expectation of our training y's. \n",
    "    - ![](dnn15.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
