{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDP vs Reinforcement Learning**\n",
    "![](rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approaches to RL**\n",
    "![](rl2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDP's Value Function**\n",
    "- $$ U(s) = R(s) + γ \\max_a \\sum_{s'}T(s, a, s') U(s')  $$\n",
    "- $$ π(s) = argmax_a \\sum_{s'}T(s, a, s') \\ U(s') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q function**\n",
    "- $$ Q(s, a) = R(s) + γ \\sum_{s'}T(s, a, s') \\max_{a'} Q(s', a')  $$\n",
    "    - Value for arriving in s\n",
    "    - Leaving via a\n",
    "    - Proceeding optimally thereafter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining MDP's Value Function as Q Functions**\n",
    "- $$U(s) = \\max_a \\ Q(s, a)$$\n",
    "- $$π(s) = argmax_a \\ Q(s, a)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-learning**\n",
    "- Evaluating the Bellman equations from data.\n",
    "    - We need to estimate Q from transitions .\n",
    "        - s, a, r, s'\n",
    "            - We are in a state, we take an action, we get the reward and we are in the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimating Q from Transitions: Q-learning Equation**\n",
    "- $$ \\hat{Q}(s, a)\\leftarrow_{\\alpha_t} \\ r + γ \\ \\max_{a'} \\hat{Q}(s', a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Intuition**: imagine if we've an estimate of the Q function.\n",
    "    - We will update by taking the state and action and moving a bit.\n",
    "    - We have the reward and discount the maximum of the utility of the next state.\n",
    "- This represents the **utility function**.\n",
    "    - $$r + γ \\ \\max_{a'} \\hat{Q}(s', a')$$\n",
    "- This represents the **utility of the next state**.\n",
    "    - $$\\max_{a'} \\hat{Q}(s', a')$$\n",
    "- $\\alpha_t$ is the **learning rate**.\n",
    "    - $ V \\leftarrow_{\\alpha_t} X $\n",
    "    - $ V \\leftarrow (1-\\alpha_t)V + \\alpha X $\n",
    "        - When $\\alpha = 0$, you would have no learning at all where your new value is your original value, V=V.\n",
    "        - When $\\alpha = 1$, you would have absolute learning where you totally forget your previous value, V leaving your new value V = X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence of Q-Learning Equation**\n",
    "- $\\hat{Q(s, a)}$ starts anywhere\n",
    "- $$ \\hat{Q}(s, a)\\leftarrow_{\\alpha_t} \\ r + γ \\ \\max_{a'} \\hat{Q}(s', a') $$\n",
    "- Then $\\hat{Q(s, a)} \\rightarrow Q(s, a)$\n",
    "    - The solution to Bellman's equation\n",
    "- But, this is true only if\n",
    "    1. (s, a) is visited infinitely often.\n",
    "    2. $\\sum_t \\alpha_t = ∞$ and $\\sum_t \\alpha^2_t < ∞$\n",
    "    3. s' ~ T(s, a, s') and r ~ R(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-learning is a family of algorithms**\n",
    "- How to initialize $\\hat{Q}$?\n",
    "- How to decay $\\alpha_t$?\n",
    "- How to choose actions?\n",
    "    - $\\epsilon$-greedy exploration\n",
    "        - If GLIE (Greedy Limit + Infinite Exploration) with decayed $\\epsilon$\n",
    "        - $\\hat{Q} \\rightarrow Q$ and $\\hat{π} \\rightarrow π^*$\n",
    "            - $\\hat{Q} \\rightarrow Q$ is learning\n",
    "            - $\\hat{π} \\rightarrow π^*$ is using\n",
    "        - We have to note that we face an exploration-exploitation dilemma here that is a fundamental tradeoff in reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Readings**\n",
    "- [Actual Proof of Q-Learning](http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf)\n",
    "- [Reinforcement Learning Paper by CMU](http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
